\select@language {english}
\contentsline {section}{\numberline {1}Introduction}{9}{section.1}
\contentsline {subsection}{\numberline {1.1}Overview}{9}{subsection.1.1}
\contentsline {subsection}{\numberline {1.2}Motivation}{10}{subsection.1.2}
\contentsline {subsection}{\numberline {1.3}State of the Art}{11}{subsection.1.3}
\contentsline {subsubsection}{\numberline {1.3.1}Dialog Policy trained with Reinforcement Learning}{11}{subsubsection.1.3.1}
\contentsline {subsubsection}{\numberline {1.3.2}Dialog Policy trained with End-to-End Supervised Learning}{13}{subsubsection.1.3.2}
\contentsline {subsubsection}{\numberline {1.3.3}Dialog Policy trained with a Combined Approach}{14}{subsubsection.1.3.3}
\contentsline {section}{\numberline {2}Dialog Systems}{15}{section.2}
\contentsline {subsection}{\numberline {2.1}Overview}{15}{subsection.2.1}
\contentsline {subsection}{\numberline {2.2}System Architecture}{15}{subsection.2.2}
\contentsline {subsection}{\numberline {2.3}Dialog Management}{16}{subsection.2.3}
\contentsline {subsubsection}{\numberline {2.3.1}Dialog State Tracking (DST)}{17}{subsubsection.2.3.1}
\contentsline {paragraph}{\numberline {2.3.1.1}Information State DST}{17}{paragraph.2.3.1.1}
\contentsline {paragraph}{\numberline {2.3.1.2}Generative DST}{17}{paragraph.2.3.1.2}
\contentsline {paragraph}{\numberline {2.3.1.3}Discriminative DST}{18}{paragraph.2.3.1.3}
\contentsline {paragraph}{\numberline {2.3.1.4}Our Approach}{19}{paragraph.2.3.1.4}
\contentsline {subsubsection}{\numberline {2.3.2}Dialog Policy}{19}{subsubsection.2.3.2}
\contentsline {paragraph}{\numberline {2.3.2.1}Our Approach}{20}{paragraph.2.3.2.1}
\contentsline {subsection}{\numberline {2.4}User Simulation}{20}{subsection.2.4}
\contentsline {subsubsection}{\numberline {2.4.1}Rule Based Simulation}{21}{subsubsection.2.4.1}
\contentsline {subsubsection}{\numberline {2.4.2}Probabilistic Model Based Simulation}{21}{subsubsection.2.4.2}
\contentsline {subsubsection}{\numberline {2.4.3}Our Approach}{22}{subsubsection.2.4.3}
\contentsline {subsection}{\numberline {2.5}Applications}{22}{subsection.2.5}
\contentsline {section}{\numberline {3}Technical Background}{24}{section.3}
\contentsline {subsection}{\numberline {3.1}Overview}{24}{subsection.3.1}
\contentsline {subsection}{\numberline {3.2}Reinforcement Learning (RL)}{24}{subsection.3.2}
\contentsline {subsection}{\numberline {3.3}Markov Decision Processes (MDP)}{26}{subsection.3.3}
\contentsline {subsubsection}{\numberline {3.3.1}Optimality}{28}{subsubsection.3.3.1}
\contentsline {subsection}{\numberline {3.4}Composing Dialog Management as an MDP}{30}{subsection.3.4}
\contentsline {subsection}{\numberline {3.5}Policy Optimization}{35}{subsection.3.5}
\contentsline {subsubsection}{\numberline {3.5.1}Value Iterative Methods}{36}{subsubsection.3.5.1}
\contentsline {paragraph}{\numberline {3.5.1.1}Dynamic Programming}{37}{paragraph.3.5.1.1}
\contentsline {paragraph}{\numberline {3.5.1.2}Monte Carlo Learning }{37}{paragraph.3.5.1.2}
\contentsline {paragraph}{\numberline {3.5.1.3}Temporal Difference(TD) Learning }{38}{paragraph.3.5.1.3}
\contentsline {paragraph}{\numberline {3.5.1.4}Sample Efficiency}{39}{paragraph.3.5.1.4}
\contentsline {paragraph}{\numberline {3.5.1.5}Function Approximation}{41}{paragraph.3.5.1.5}
\contentsline {subsubsection}{\numberline {3.5.2}Policy Iterative Methods}{41}{subsubsection.3.5.2}
\contentsline {subsubsection}{\numberline {3.5.3}Actor-Critic Methods}{44}{subsubsection.3.5.3}
\contentsline {subsection}{\numberline {3.6}Evaluation and Reward Estimation}{46}{subsection.3.6}
\contentsline {subsubsection}{\numberline {3.6.1}Heuristic Rewards}{47}{subsubsection.3.6.1}
\contentsline {subsubsection}{\numberline {3.6.2}The Paradise Framework}{47}{subsubsection.3.6.2}
\contentsline {subsubsection}{\numberline {3.6.3}Reward Shaping}{47}{subsubsection.3.6.3}
\contentsline {subsection}{\numberline {3.7}Auxiliary RL terminology}{48}{subsection.3.7}
\contentsline {subsubsection}{\numberline {3.7.1}Model-based and Model-free}{49}{subsubsection.3.7.1}
\contentsline {subsubsection}{\numberline {3.7.2}Episodic and Continuous Tasks}{49}{subsubsection.3.7.2}
\contentsline {subsubsection}{\numberline {3.7.3}On-line and Off-line}{49}{subsubsection.3.7.3}
\contentsline {subsubsection}{\numberline {3.7.4}On Policy and Off Policy}{50}{subsubsection.3.7.4}
\contentsline {subsubsection}{\numberline {3.7.5}Exploration and Exploitation}{50}{subsubsection.3.7.5}
\contentsline {section}{\numberline {4}Deep Learning}{51}{section.4}
\contentsline {subsection}{\numberline {4.1}Overview}{51}{subsection.4.1}
\contentsline {subsection}{\numberline {4.2}Deep Feedforward Networks}{51}{subsection.4.2}
\contentsline {subsection}{\numberline {4.3}Loss Functions}{52}{subsection.4.3}
\contentsline {subsubsection}{\numberline {4.3.1}Maximum Likelihood Estimation}{53}{subsubsection.4.3.1}
\contentsline {subsubsection}{\numberline {4.3.2}Approximating Conditional Distributions with Maximum Likelihood}{54}{subsubsection.4.3.2}
\contentsline {subsection}{\numberline {4.4}Output Units}{55}{subsection.4.4}
\contentsline {subsubsection}{\numberline {4.4.1}Linear Units}{55}{subsubsection.4.4.1}
\contentsline {subsubsection}{\numberline {4.4.2}Sigmoid Units}{55}{subsubsection.4.4.2}
\contentsline {subsubsection}{\numberline {4.4.3}Softmax Units}{56}{subsubsection.4.4.3}
\contentsline {subsection}{\numberline {4.5}Hidden Units}{57}{subsection.4.5}
\contentsline {subsubsection}{\numberline {4.5.1}Rectified Linear Units(ReLU)}{57}{subsubsection.4.5.1}
\contentsline {subsubsection}{\numberline {4.5.2}The Hyperbolic Tangent and Logistic Sigmoid}{58}{subsubsection.4.5.2}
\contentsline {subsection}{\numberline {4.6}Architecture}{60}{subsection.4.6}
\contentsline {subsection}{\numberline {4.7}Gradient Based Learning}{61}{subsection.4.7}
\contentsline {subsubsection}{\numberline {4.7.1}Back-Propagation}{62}{subsubsection.4.7.1}
\contentsline {subsubsection}{\numberline {4.7.2}Stochastic Gradient Descent(SGD)}{64}{subsubsection.4.7.2}
\contentsline {section}{\numberline {5}Neural Dialog Management}{66}{section.5}
\contentsline {subsection}{\numberline {5.1}Overview}{66}{subsection.5.1}
\contentsline {subsection}{\numberline {5.2}REINFORCE}{66}{subsection.5.2}
\contentsline {subsubsection}{\numberline {5.2.1}REINFORCE Algorithm}{68}{subsubsection.5.2.1}
\contentsline {subsubsection}{\numberline {5.2.2}Policy Network}{68}{subsubsection.5.2.2}
\contentsline {subsection}{\numberline {5.3}Advantage Actor Critic}{70}{subsection.5.3}
\contentsline {subsubsection}{\numberline {5.3.1}Advantage Function}{70}{subsubsection.5.3.1}
\contentsline {subsubsection}{\numberline {5.3.2}A2C Algorithm}{72}{subsubsection.5.3.2}
\contentsline {subsubsection}{\numberline {5.3.3}Actor Network}{72}{subsubsection.5.3.3}
\contentsline {subsubsection}{\numberline {5.3.4}Critic Network}{74}{subsubsection.5.3.4}
\contentsline {section}{\numberline {6}Experiments and Results}{76}{section.6}
\contentsline {subsection}{\numberline {6.1}Overview}{76}{subsection.6.1}
\contentsline {subsection}{\numberline {6.2}Learning Simple Dialog Strategies}{76}{subsection.6.2}
\contentsline {subsubsection}{\numberline {6.2.1}Experimental Setup}{76}{subsubsection.6.2.1}
\contentsline {subsubsection}{\numberline {6.2.2}Learning with A2C}{81}{subsubsection.6.2.2}
\contentsline {paragraph}{\numberline {6.2.2.1}Results}{82}{paragraph.6.2.2.1}
\contentsline {paragraph}{\numberline {6.2.2.2}Effect of the Discount Factor}{84}{paragraph.6.2.2.2}
\contentsline {paragraph}{\numberline {6.2.2.3}Effect of Reward Magnitude}{86}{paragraph.6.2.2.3}
\contentsline {subsubsection}{\numberline {6.2.3}Learning with REINFORCE}{88}{subsubsection.6.2.3}
\contentsline {paragraph}{\numberline {6.2.3.1}Results}{88}{paragraph.6.2.3.1}
\contentsline {subsection}{\numberline {6.3}Summary}{90}{subsection.6.3}
\contentsline {section}{\numberline {7}Conclusions}{92}{section.7}
\contentsline {subsection}{\numberline {7.1}Conclusion}{92}{subsection.7.1}
\contentsline {subsection}{\numberline {7.2}Future Work}{92}{subsection.7.2}
\contentsline {section}{\numberline {8}References}{95}{section.8}
