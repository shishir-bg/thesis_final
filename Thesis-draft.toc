\select@language {english}
\contentsline {section}{\numberline {1}Introduction}{10}{section.1}
\contentsline {subsection}{\numberline {1.1}Overview}{10}{subsection.1.1}
\contentsline {subsection}{\numberline {1.2}Motivation}{11}{subsection.1.2}
\contentsline {subsection}{\numberline {1.3}State of the Art}{12}{subsection.1.3}
\contentsline {subsubsection}{\numberline {1.3.1}Dialog Policy trained with Reinforcement Learning}{12}{subsubsection.1.3.1}
\contentsline {subsubsection}{\numberline {1.3.2}Dialog Policy trained with End-to-End Supervised Learning}{13}{subsubsection.1.3.2}
\contentsline {subsubsection}{\numberline {1.3.3}Dialog Policy trained with a Combined Approach}{14}{subsubsection.1.3.3}
\contentsline {section}{\numberline {2}Dialog Systems}{16}{section.2}
\contentsline {subsection}{\numberline {2.1}Overview}{16}{subsection.2.1}
\contentsline {subsection}{\numberline {2.2}System Architecture}{16}{subsection.2.2}
\contentsline {subsection}{\numberline {2.3}Dialog Management}{17}{subsection.2.3}
\contentsline {subsubsection}{\numberline {2.3.1}Dialog State Tracking (DST)}{18}{subsubsection.2.3.1}
\contentsline {paragraph}{\numberline {2.3.1.1}Information State DST}{18}{paragraph.2.3.1.1}
\contentsline {paragraph}{\numberline {2.3.1.2}Generative DST}{18}{paragraph.2.3.1.2}
\contentsline {paragraph}{\numberline {2.3.1.3}Discriminative DST}{19}{paragraph.2.3.1.3}
\contentsline {paragraph}{\numberline {2.3.1.4}Our Approach}{20}{paragraph.2.3.1.4}
\contentsline {subsubsection}{\numberline {2.3.2}Dialog Policy}{20}{subsubsection.2.3.2}
\contentsline {paragraph}{\numberline {2.3.2.1}Our Approach}{21}{paragraph.2.3.2.1}
\contentsline {subsection}{\numberline {2.4}User Simulation}{21}{subsection.2.4}
\contentsline {subsubsection}{\numberline {2.4.1}Rule Based Simulation}{22}{subsubsection.2.4.1}
\contentsline {subsubsection}{\numberline {2.4.2}Probabilistic Model Based Simulation}{22}{subsubsection.2.4.2}
\contentsline {subsubsection}{\numberline {2.4.3}Our Approach}{23}{subsubsection.2.4.3}
\contentsline {subsection}{\numberline {2.5}Applications}{23}{subsection.2.5}
\contentsline {section}{\numberline {3}Technical Background}{24}{section.3}
\contentsline {subsection}{\numberline {3.1}Overview}{24}{subsection.3.1}
\contentsline {subsection}{\numberline {3.2}Reinforcement Learning (RL)}{24}{subsection.3.2}
\contentsline {subsection}{\numberline {3.3}Markov Decision Processes (MDP)}{26}{subsection.3.3}
\contentsline {subsubsection}{\numberline {3.3.1}Optimality}{28}{subsubsection.3.3.1}
\contentsline {subsection}{\numberline {3.4}Composing Dialog Management as an MDP}{29}{subsection.3.4}
\contentsline {subsection}{\numberline {3.5}Policy Optimization}{35}{subsection.3.5}
\contentsline {subsubsection}{\numberline {3.5.1}Value Iterative Methods}{36}{subsubsection.3.5.1}
\contentsline {paragraph}{\numberline {3.5.1.1}Dynamic Programming}{36}{paragraph.3.5.1.1}
\contentsline {paragraph}{\numberline {3.5.1.2}Monte Carlo Learning }{37}{paragraph.3.5.1.2}
\contentsline {paragraph}{\numberline {3.5.1.3}Temporal Difference(TD) Learning }{37}{paragraph.3.5.1.3}
\contentsline {paragraph}{\numberline {3.5.1.4}Sample Efficiency}{39}{paragraph.3.5.1.4}
\contentsline {paragraph}{\numberline {3.5.1.5}Function Approximation}{40}{paragraph.3.5.1.5}
\contentsline {subsubsection}{\numberline {3.5.2}Policy Iterative Methods}{41}{subsubsection.3.5.2}
\contentsline {subsubsection}{\numberline {3.5.3}Actor-Critic Methods}{44}{subsubsection.3.5.3}
\contentsline {subsection}{\numberline {3.6}Evaluation and Reward Estimation}{46}{subsection.3.6}
\contentsline {subsubsection}{\numberline {3.6.1}Heuristic Rewards}{46}{subsubsection.3.6.1}
\contentsline {subsubsection}{\numberline {3.6.2}The Paradise Framework}{46}{subsubsection.3.6.2}
\contentsline {subsubsection}{\numberline {3.6.3}Reward Shaping}{47}{subsubsection.3.6.3}
\contentsline {subsection}{\numberline {3.7}Auxiliary RL terminology}{48}{subsection.3.7}
\contentsline {subsubsection}{\numberline {3.7.1}Model-based and Model-free}{48}{subsubsection.3.7.1}
\contentsline {subsubsection}{\numberline {3.7.2}Episodic and Continuous Tasks}{48}{subsubsection.3.7.2}
\contentsline {subsubsection}{\numberline {3.7.3}On-line and Off-line}{49}{subsubsection.3.7.3}
\contentsline {subsubsection}{\numberline {3.7.4}On Policy and Off Policy}{49}{subsubsection.3.7.4}
\contentsline {subsubsection}{\numberline {3.7.5}Exploration and Exploitation}{49}{subsubsection.3.7.5}
\contentsline {section}{\numberline {4}Deep Learning}{51}{section.4}
\contentsline {subsection}{\numberline {4.1}Overview}{51}{subsection.4.1}
\contentsline {subsection}{\numberline {4.2}Deep Feedforward Networks}{51}{subsection.4.2}
\contentsline {subsection}{\numberline {4.3}Loss Functions}{52}{subsection.4.3}
\contentsline {subsubsection}{\numberline {4.3.1}Maximum Likelihood Estimation}{53}{subsubsection.4.3.1}
\contentsline {subsubsection}{\numberline {4.3.2}Approximating Conditional Distributions with Maximum Likelihood}{54}{subsubsection.4.3.2}
\contentsline {subsection}{\numberline {4.4}Output Units}{55}{subsection.4.4}
\contentsline {subsubsection}{\numberline {4.4.1}Linear Units}{55}{subsubsection.4.4.1}
\contentsline {subsubsection}{\numberline {4.4.2}Sigmoid Units}{55}{subsubsection.4.4.2}
\contentsline {subsubsection}{\numberline {4.4.3}Softmax Units}{56}{subsubsection.4.4.3}
\contentsline {subsection}{\numberline {4.5}Hidden Units}{56}{subsection.4.5}
\contentsline {subsubsection}{\numberline {4.5.1}Rectified Linear Units(ReLU)}{57}{subsubsection.4.5.1}
\contentsline {subsubsection}{\numberline {4.5.2}The Hyperbolic Tangent and Logistic Sigmoid}{58}{subsubsection.4.5.2}
\contentsline {subsection}{\numberline {4.6}Architecture}{60}{subsection.4.6}
\contentsline {subsection}{\numberline {4.7}Gradient Based Learning}{61}{subsection.4.7}
\contentsline {subsubsection}{\numberline {4.7.1}Back-Propagation}{62}{subsubsection.4.7.1}
\contentsline {subsubsection}{\numberline {4.7.2}Stochastic Gradient Descent(SGD)}{63}{subsubsection.4.7.2}
\contentsline {section}{\numberline {5}Neural Dialog Management}{65}{section.5}
\contentsline {subsection}{\numberline {5.1}Overview}{65}{subsection.5.1}
\contentsline {subsection}{\numberline {5.2}REINFORCE}{65}{subsection.5.2}
\contentsline {subsubsection}{\numberline {5.2.1}REINFORCE Algorithm}{67}{subsubsection.5.2.1}
\contentsline {subsubsection}{\numberline {5.2.2}Policy Network}{67}{subsubsection.5.2.2}
\contentsline {subsection}{\numberline {5.3}Advantage Actor Critic}{68}{subsection.5.3}
\contentsline {subsubsection}{\numberline {5.3.1}Advantage Function}{69}{subsubsection.5.3.1}
\contentsline {subsubsection}{\numberline {5.3.2}A2C Algorithm}{70}{subsubsection.5.3.2}
\contentsline {subsubsection}{\numberline {5.3.3}Actor Network}{72}{subsubsection.5.3.3}
\contentsline {subsubsection}{\numberline {5.3.4}Critic Network}{74}{subsubsection.5.3.4}
\contentsline {section}{\numberline {6}Experiments and Results}{76}{section.6}
\contentsline {subsection}{\numberline {6.1}Overview}{76}{subsection.6.1}
\contentsline {subsection}{\numberline {6.2}Learning Simple Dialog Strategies}{76}{subsection.6.2}
\contentsline {subsubsection}{\numberline {6.2.1}Experimental Setup}{76}{subsubsection.6.2.1}
\contentsline {subsubsection}{\numberline {6.2.2}Learning with A2C}{81}{subsubsection.6.2.2}
\contentsline {paragraph}{\numberline {6.2.2.1}Results}{82}{paragraph.6.2.2.1}
\contentsline {paragraph}{\numberline {6.2.2.2}Effect of the Discount Factor}{84}{paragraph.6.2.2.2}
\contentsline {paragraph}{\numberline {6.2.2.3}Effect of Reward Magnitude}{86}{paragraph.6.2.2.3}
\contentsline {subsubsection}{\numberline {6.2.3}Learning with REINFORCE}{88}{subsubsection.6.2.3}
\contentsline {paragraph}{\numberline {6.2.3.1}Results}{88}{paragraph.6.2.3.1}
\contentsline {subsection}{\numberline {6.3}Summary}{90}{subsection.6.3}
\contentsline {section}{\numberline {7}Conclusions}{92}{section.7}
\contentsline {subsection}{\numberline {7.1}Conclusion}{92}{subsection.7.1}
\contentsline {subsection}{\numberline {7.2}Future Work}{92}{subsection.7.2}
\contentsline {section}{\numberline {8}Abbreviations}{94}{section.8}
\contentsline {section}{\numberline {9}References}{95}{section.9}
